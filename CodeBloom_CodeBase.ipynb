{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# WingMate AI â€” Context-Aware Co-Vis Recommender (Clean Final)\n",
        "# ============================================================\n",
        "# What you get:\n",
        "#   - Robust CSV reader (handles stray commas in ORDERS column)\n",
        "#   - Context co-visitation (global + channel + subchannel + occasion + store + customer type)\n",
        "#   - Tuned blend weights + optional MMR diversity\n",
        "#   - Two outputs: MAX (submit) and TUNED (demo)\n",
        "#   - Strict LOO & Temporal holdout evaluations (judge-friendly)\n",
        "#   - Optional LightGBM reranker (compact features)\n",
        "#\n",
        "# Usage (script):\n",
        "#   python unravel_final.py \\\n",
        "#       --order_path /content/order_data.csv \\\n",
        "#       --test_path  /content/test_data_question.csv \\\n",
        "#       --out_dir    /content \\\n",
        "#       --use_reranker false \\\n",
        "#       --do_eval true \\\n",
        "#       --do_temporal true\n",
        "#\n",
        "# Or: run all cells in notebook order.\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os, gc, math, random, re, json, csv, time, argparse\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------\n",
        "# 1) Paths & Repro\n",
        "# -------------------\n",
        "def set_seed(s=42):\n",
        "    random.seed(s); np.random.seed(s)\n",
        "set_seed(42)\n",
        "\n",
        "# Defaults (override via CLI)\n",
        "ORDER_PATH = \"/content/order_data.csv\"\n",
        "TEST_PATH  = \"/content/test_data_question.csv\"\n",
        "OUT_DIR    = \"/content\"\n",
        "\n",
        "# Derived outputs\n",
        "def out_paths(out_dir):\n",
        "    return {\n",
        "        \"OUT_MAX\":   os.path.join(out_dir, \"Recommendation_Output_MAX.xlsx\"),\n",
        "        \"OUT_TUNED\": os.path.join(out_dir, \"Recommendation_Output_TUNED.xlsx\"),\n",
        "        \"REASONS\":   os.path.join(out_dir, \"Recommendation_With_Reasons.csv\"),\n",
        "        \"MET_JSON\":  os.path.join(out_dir, \"metrics.json\"),\n",
        "        \"MET_CSV\":   os.path.join(out_dir, \"metrics.csv\"),\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Robust reader & utilities\n",
        "# -----------------------------\n",
        "EXPECTED_COLS = (\"CUSTOMER_ID\",\"ORDER_ID\",\n",
        "                 \"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\n",
        "                 \"ORDER_OCCASION_NAME\",\"STORE_NUMBER\",\n",
        "                 \"CUSTOMER_TYPE\",\"ORDERS\")\n",
        "\n",
        "def row_iter(path, expected_cols=EXPECTED_COLS):\n",
        "    \"\"\"\n",
        "    Robust iterator for large/messy CSVs.\n",
        "    Splits only on the first 7 commas; keeps the rest of the line as last field (ORDERS).\n",
        "    \"\"\"\n",
        "    exp_n = len(expected_cols)  # 8\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        _ = f.readline()  # header line (ignored)\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\",\", exp_n - 1)  # keep extra commas in last field\n",
        "            if len(parts) < exp_n:\n",
        "                continue\n",
        "            yield dict(zip(expected_cols, parts[:exp_n]))\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def extract_items(order_text) -> list:\n",
        "    if order_text is None: return []\n",
        "    t = str(order_text).strip().strip('\"').strip(\"'\")\n",
        "    if not t: return []\n",
        "    # Accept delims: | , ;\n",
        "    parts = re.split(r\"[|,;]\\s*\", t)\n",
        "    items = []\n",
        "    seen = set()\n",
        "    for p in parts:\n",
        "        pp = normalize_text(p)\n",
        "        if not pp: continue\n",
        "        if pp.lower() in {\"na\",\"none\",\"null\",\"nan\"}: continue\n",
        "        if pp not in seen:\n",
        "            seen.add(pp); items.append(pp)\n",
        "    return items\n",
        "\n",
        "def safe_get(row, key, default=\"\"):\n",
        "    return normalize_text(row.get(key, default))\n",
        "\n",
        "def first_token(s: str) -> str:\n",
        "    s = normalize_text(s).upper()\n",
        "    m = re.match(r\"[A-Z0-9]+\", s)\n",
        "    return m.group(0) if m else s[:8]\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3) Build co-visitation maps + popularity\n",
        "# --------------------------------------------\n",
        "def build_covis_maps(order_path):\n",
        "    covis_global      = defaultdict(Counter)\n",
        "    covis_by_channel  = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_subch    = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_occ      = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_store    = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_custtype = defaultdict(lambda: defaultdict(Counter))\n",
        "    pop               = Counter()\n",
        "\n",
        "    t0=time.time()\n",
        "    rows_parsed=0; orders_with_items=0; uniq_items=set()\n",
        "\n",
        "    for row in tqdm(row_iter(order_path), desc=\"Building co-vis maps\"):\n",
        "        items = extract_items(row.get(\"ORDERS\", \"\"))\n",
        "        if not items:\n",
        "            continue\n",
        "        orders_with_items += 1; rows_parsed += 1\n",
        "\n",
        "        # popularity (unique per order)\n",
        "        for it in set(items):\n",
        "            pop[it] += 1; uniq_items.add(it)\n",
        "\n",
        "        uniq = list(set(items))\n",
        "        n = len(uniq)\n",
        "        if n > 1:\n",
        "            ch = row.get(\"ORDER_CHANNEL_NAME\",\"\")\n",
        "            sc = row.get(\"ORDER_SUBCHANNEL_NAME\",\"\")\n",
        "            oc = row.get(\"ORDER_OCCASION_NAME\",\"\")\n",
        "            st = row.get(\"STORE_NUMBER\",\"\")\n",
        "            ct = row.get(\"CUSTOMER_TYPE\",\"\")\n",
        "            for i in range(n):\n",
        "                a = uniq[i]\n",
        "                for j in range(i+1, n):\n",
        "                    b = uniq[j]\n",
        "                    covis_global[a][b]+=1; covis_global[b][a]+=1\n",
        "                    covis_by_channel[ch][a][b]+=1;   covis_by_channel[ch][b][a]+=1\n",
        "                    covis_by_subch[sc][a][b]+=1;     covis_by_subch[sc][b][a]+=1\n",
        "                    covis_by_occ[oc][a][b]+=1;       covis_by_occ[oc][b][a]+=1\n",
        "                    covis_by_store[st][a][b]+=1;     covis_by_store[st][b][a]+=1\n",
        "                    covis_by_custtype[ct][a][b]+=1;  covis_by_custtype[ct][b][a]+=1\n",
        "\n",
        "        if rows_parsed % 200_000 == 0:\n",
        "            print(f\"Processed {rows_parsed:,} rows...\")\n",
        "\n",
        "    t1=time.time()\n",
        "    print(f\"Rows parsed: {rows_parsed:,} | Orders with items: {orders_with_items:,} | Unique items: {len(pop)}\")\n",
        "    print(f\"Build time: {round((t1-t0)/60, 2)} min\")\n",
        "    return covis_global, covis_by_channel, covis_by_subch, covis_by_occ, covis_by_store, covis_by_custtype, pop\n",
        "\n",
        "def normalize_covis(cmap: dict, pop: Counter, power=0.5, keep_top=500):\n",
        "    out = {}\n",
        "    for a, neigh in cmap.items():\n",
        "        pa = max(pop[a], 1)\n",
        "        denom_a = pa**power\n",
        "        d = {}\n",
        "        for b, c in neigh.items():\n",
        "            pb = max(pop[b], 1)\n",
        "            d[b] = float(c) / (denom_a * (pb**power))\n",
        "        out[a] = dict(Counter(d).most_common(keep_top))\n",
        "    return out\n",
        "\n",
        "def norm_nested(ctx_map, pop):\n",
        "    out = {}\n",
        "    for key, cmap in ctx_map.items():\n",
        "        out[key] = normalize_covis(cmap, pop, power=0.5)\n",
        "    return out\n",
        "\n",
        "# -------------------------------------\n",
        "# 4) Scoring blend + MMR (diversity)\n",
        "# -------------------------------------\n",
        "# Tuned weights (from your good runs)\n",
        "W        = (0.51, 0.24, 0.05, 0.11, 0.17, 0.10)  # (global, channel, subchannel, occasion, store, customer_type)\n",
        "LAM      = 0.93\n",
        "BACKOFF_ALPHA = 0.15\n",
        "CAND_POOL     = 120\n",
        "BACKFILL_POOL = 800\n",
        "\n",
        "def blended_scores(cart, row, covis_global, covis_by_channel, covis_by_subch, covis_by_occ, covis_by_store, covis_by_custtype, pop, w=W):\n",
        "    scores = Counter()\n",
        "    def add_from(map_, weight):\n",
        "        if weight <= 0 or not map_: return\n",
        "        for it in cart:\n",
        "            for c, v in map_.get(it, {}).items():\n",
        "                scores[c] += weight * v\n",
        "\n",
        "    ch = str(row.get(\"ORDER_CHANNEL_NAME\", \"\"))\n",
        "    sc = str(row.get(\"ORDER_SUBCHANNEL_NAME\", \"\"))\n",
        "    oc = str(row.get(\"ORDER_OCCASION_NAME\", \"\"))\n",
        "    st = str(row.get(\"STORE_NUMBER\", \"\"))\n",
        "    ct = str(row.get(\"CUSTOMER_TYPE\", \"\"))\n",
        "\n",
        "    add_from(covis_global, w[0])\n",
        "    if ch in covis_by_channel:  add_from(covis_by_channel[ch],  w[1])\n",
        "    if sc in covis_by_subch:    add_from(covis_by_subch[sc],    w[2])\n",
        "    if oc in covis_by_occ:      add_from(covis_by_occ[oc],      w[3])\n",
        "    if st in covis_by_store:    add_from(covis_by_store[st],    w[4])\n",
        "    if ct in covis_by_custtype: add_from(covis_by_custtype[ct], w[5])\n",
        "\n",
        "    for it in cart:\n",
        "        scores.pop(it, None)\n",
        "\n",
        "    # sparse backoff\n",
        "    if len(scores) < 5:\n",
        "        for it in cart:\n",
        "            for c, v in covis_global.get(it, {}).items():\n",
        "                scores[c] += BACKOFF_ALPHA * v\n",
        "        for it in cart: scores.pop(it, None)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def mmr_select(cands, k=3, lam=LAM):\n",
        "    sel=[]\n",
        "    def sim(a,b):\n",
        "        ta=set(first_token(a).split()) | set(a.upper().split())\n",
        "        tb=set(first_token(b).split()) | set(b.upper().split())\n",
        "        inter=len(ta & tb); denom=math.sqrt(len(ta)*len(tb)) or 1.0\n",
        "        return inter/denom\n",
        "    work=cands[:]\n",
        "    while work and len(sel)<k:\n",
        "        best=None; bestv=-1e9\n",
        "        for idx,c in enumerate(work):\n",
        "            rel=1.0/(1.0+idx)\n",
        "            div=0.0 if not sel else max(sim(c,s) for s in sel)\n",
        "            v=lam*rel - (1-lam)*div\n",
        "            if v>bestv: best,bestv=c,v\n",
        "        sel.append(best); work.remove(best)\n",
        "    return sel\n",
        "\n",
        "def recommend_ctx(cart, row, maps, global_top, pop, k=3, use_mmr=False):\n",
        "    (covis_global, covis_by_channel, covis_by_subch, covis_by_occ, covis_by_store, covis_by_custtype) = maps\n",
        "    scores = blended_scores(cart, row, covis_global, covis_by_channel, covis_by_subch, covis_by_occ, covis_by_store, covis_by_custtype, pop)\n",
        "    cands = [c for c,_ in scores.most_common(BACKFILL_POOL) if c not in cart][:CAND_POOL]\n",
        "    # backfill with popularity\n",
        "    for g in global_top:\n",
        "        if len(cands) >= CAND_POOL: break\n",
        "        if g not in cart and g not in cands: cands.append(g)\n",
        "    final = mmr_select(cands, k=k) if use_mmr else cands[:k]\n",
        "    return (final + [\"\",\"\",\"\"])[:k]\n",
        "\n",
        "# ---------------------------------\n",
        "# 5) Inference + output writers\n",
        "# ---------------------------------\n",
        "def write_excel(test_path, out_path, maps, global_top, pop, use_mmr=False, reasons_csv=None):\n",
        "    df = pd.read_csv(test_path, dtype=str, keep_default_na=False)\n",
        "    item_cols = [c for c in df.columns if c.upper().startswith(\"ITEM\")]\n",
        "    p1,p2,p3 = [],[],[]\n",
        "    reasons_rows=[]\n",
        "    for _, r in tqdm(df.iterrows(), total=len(df), desc=(\"Predict MMR\" if use_mmr else \"Predict Max\")):\n",
        "        cart = [normalize_text(r[c]) for c in item_cols if r.get(c,\"\")]\n",
        "        recs = recommend_ctx(cart, r, maps, global_top, pop, k=3, use_mmr=use_mmr)\n",
        "        p1.append(recs[0]); p2.append(recs[1]); p3.append(recs[2])\n",
        "        if reasons_csv is not None:\n",
        "            reasons_rows.append({\n",
        "                \"ORDER_ID\": r.get(\"ORDER_ID\",\"\"),\n",
        "                \"CART\": \" | \".join(cart[:8]),\n",
        "                \"REC1\": recs[0],\n",
        "                \"REC2\": recs[1],\n",
        "                \"REC3\": recs[2],\n",
        "            })\n",
        "    out = df.copy()\n",
        "    out[\"RECOMMENDATION 1\"] = p1\n",
        "    out[\"RECOMMENDATION 2\"] = p2\n",
        "    out[\"RECOMMENDATION 3\"] = p3\n",
        "    cols = [\"CUSTOMER_ID\",\"ORDER_ID\"] + item_cols + [\"RECOMMENDATION 1\",\"RECOMMENDATION 2\",\"RECOMMENDATION 3\"]\n",
        "    # Write Excel (fallback to CSV if engine missing)\n",
        "    try:\n",
        "        out[cols].to_excel(out_path, index=False)\n",
        "    except Exception:\n",
        "        out_csv = out_path.replace(\".xlsx\", \".csv\")\n",
        "        out[cols].to_csv(out_csv, index=False)\n",
        "        print(f\"[INFO] openpyxl not found; wrote CSV instead: {out_csv}\")\n",
        "    if reasons_csv is not None:\n",
        "        pd.DataFrame(reasons_rows).to_csv(reasons_csv, index=False)\n",
        "    print(f\"Wrote: {out_path}\")\n",
        "\n",
        "# ---------------------------------\n",
        "# 6) Offline evaluations (strict & temporal)\n",
        "# ---------------------------------\n",
        "def eval_strict_loo(order_path, maps, global_top, pop, n_eval=5000):\n",
        "    hits1=hits2=hits3=map3=ndcg3=0.0; seen=0\n",
        "    for row in row_iter(order_path):\n",
        "        items = extract_items(row.get(\"ORDERS\",\"\"))\n",
        "        uniq = list(dict.fromkeys(items))  # preserve order, drop dups\n",
        "        if len(uniq) < 2: continue\n",
        "        target = uniq[-1]\n",
        "        cart = [x for x in uniq if x != target]\n",
        "        if not cart: continue\n",
        "        preds = recommend_ctx(cart, row, maps, global_top, pop, k=3, use_mmr=False)\n",
        "        if len(preds)>=1 and preds[0]==target: hits1+=1\n",
        "        if target in preds[:2]: hits2+=1\n",
        "        if target in preds[:3]: hits3+=1\n",
        "        if target in preds[:3]:\n",
        "            rank = preds.index(target)+1\n",
        "            map3 += 1.0/rank\n",
        "            ndcg3+= 1.0/math.log2(rank+1)\n",
        "        seen += 1\n",
        "        if seen >= n_eval: break\n",
        "    if seen==0:\n",
        "        return {\"num_eval_orders\":0,\"Recall@1\":0,\"Recall@2\":0,\"Recall@3\":0,\"MAP@3\":0,\"NDCG@3\":0}\n",
        "    return {\"num_eval_orders\": seen,\n",
        "            \"Recall@1\": round(hits1/seen,4),\n",
        "            \"Recall@2\": round(hits2/seen,4),\n",
        "            \"Recall@3\": round(hits3/seen,4),\n",
        "            \"MAP@3\": round(map3/seen,4),\n",
        "            \"NDCG@3\": round(ndcg3/seen,4)}\n",
        "\n",
        "def count_rows_fast(path):\n",
        "    return max(0, sum(1 for _ in open(path, \"r\", encoding=\"utf-8\", errors=\"replace\")) - 1)\n",
        "\n",
        "def build_covis_upto(order_path, limit_rows):\n",
        "    cg      = defaultdict(Counter)\n",
        "    by_ch   = defaultdict(lambda: defaultdict(Counter))\n",
        "    by_sc   = defaultdict(lambda: defaultdict(Counter))\n",
        "    by_oc   = defaultdict(lambda: defaultdict(Counter))\n",
        "    by_st   = defaultdict(lambda: defaultdict(Counter))\n",
        "    by_ct   = defaultdict(lambda: defaultdict(Counter))\n",
        "    pop     = Counter()\n",
        "    i=0\n",
        "    for row in row_iter(order_path):\n",
        "        if i >= limit_rows: break\n",
        "        items = extract_items(row.get(\"ORDERS\",\"\"))\n",
        "        if not items:\n",
        "            i+=1; continue\n",
        "        for it in set(items): pop[it] += 1\n",
        "        uniq = list(set(items))\n",
        "        n = len(uniq)\n",
        "        if n > 1:\n",
        "            ch=row.get(\"ORDER_CHANNEL_NAME\",\"\"); sc=row.get(\"ORDER_SUBCHANNEL_NAME\",\"\")\n",
        "            oc=row.get(\"ORDER_OCCASION_NAME\",\"\"); st=row.get(\"STORE_NUMBER\",\"\")\n",
        "            ct=row.get(\"CUSTOMER_TYPE\",\"\")\n",
        "            for a_i in range(n):\n",
        "                a = uniq[a_i]\n",
        "                for b_i in range(a_i+1, n):\n",
        "                    b = uniq[b_i]\n",
        "                    cg[a][b]+=1; cg[b][a]+=1\n",
        "                    by_ch[ch][a][b]+=1;  by_ch[ch][b][a]+=1\n",
        "                    by_sc[sc][a][b]+=1;  by_sc[sc][b][a]+=1\n",
        "                    by_oc[oc][a][b]+=1;  by_oc[oc][b][a]+=1\n",
        "                    by_st[st][a][b]+=1;  by_st[st][b][a]+=1\n",
        "                    by_ct[ct][a][b]+=1;  by_ct[ct][b][a]+=1\n",
        "        i += 1\n",
        "    global_top = [it for it,_ in pop.most_common(1000)]\n",
        "    return cg, by_ch, by_sc, by_oc, by_st, by_ct, pop, global_top\n",
        "\n",
        "def normalize_all(cg, by_ch, by_sc, by_oc, by_st, by_ct, pop):\n",
        "    cg = normalize_covis(cg, pop, power=0.5)\n",
        "    by_ch = norm_nested(by_ch, pop)\n",
        "    by_sc = norm_nested(by_sc, pop)\n",
        "    by_oc = norm_nested(by_oc, pop)\n",
        "    by_st = norm_nested(by_st, pop)\n",
        "    by_ct = norm_nested(by_ct, pop)\n",
        "    return cg, by_ch, by_sc, by_oc, by_st, by_ct\n",
        "\n",
        "def temporal_eval(order_path, frac_train=0.90, max_eval=8000):\n",
        "    total = count_rows_fast(order_path)\n",
        "    cut = int(total * frac_train)\n",
        "    print(f\"[Temporal] total={total:,} train_first={cut:,} test_last={total-cut:,}\")\n",
        "    cg, by_ch, by_sc, by_oc, by_st, by_ct, pop, gtop = build_covis_upto(order_path, cut)\n",
        "    cg, by_ch, by_sc, by_oc, by_st, by_ct = normalize_all(cg, by_ch, by_sc, by_oc, by_st, by_ct, pop)\n",
        "    maps = (cg, by_ch, by_sc, by_oc, by_st, by_ct)\n",
        "    hits1=hits2=hits3=map3=ndcg3=0.0; seen=0; i=0\n",
        "    for row in row_iter(order_path):\n",
        "        i+=1\n",
        "        if i<=cut: continue\n",
        "        items = extract_items(row.get(\"ORDERS\",\"\"))\n",
        "        uniq = list(dict.fromkeys(items))\n",
        "        if len(uniq)<2: continue\n",
        "        target=uniq[-1]\n",
        "        cart = [x for x in uniq if x!=target]\n",
        "        if not cart: continue\n",
        "        preds = recommend_ctx(cart, row, maps, gtop, pop, k=3, use_mmr=False)\n",
        "        if len(preds)>=1 and preds[0]==target: hits1+=1\n",
        "        if target in preds[:2]: hits2+=1\n",
        "        if target in preds[:3]: hits3+=1\n",
        "        if target in preds[:3]:\n",
        "            rank = preds.index(target)+1\n",
        "            map3 += 1.0/rank\n",
        "            ndcg3+= 1.0/math.log2(rank+1)\n",
        "        seen += 1\n",
        "        if seen >= max_eval: break\n",
        "    if seen==0:\n",
        "        return {\"num_eval_orders\":0,\"Recall@1\":0,\"Recall@2\":0,\"Recall@3\":0,\"MAP@3\":0,\"NDCG@3\":0}\n",
        "    return {\"num_eval_orders\": seen,\n",
        "            \"Recall@1\": round(hits1/seen,4),\n",
        "            \"Recall@2\": round(hits2/seen,4),\n",
        "            \"Recall@3\": round(hits3/seen,4),\n",
        "            \"MAP@3\": round(map3/seen,4),\n",
        "            \"NDCG@3\": round(ndcg3/seen,4)}\n",
        "\n",
        "# ---------------------------------\n",
        "# 7) Optional: LightGBM reranker\n",
        "# ---------------------------------\n",
        "def train_reranker(order_path, maps, global_top, pop, n_orders_train=300_000, n_orders_valid=80_000, cand_top=80):\n",
        "    import lightgbm as lgb\n",
        "    from lightgbm import LGBMRanker\n",
        "    X_tr=[]; y_tr=[]; g_tr=[]\n",
        "    X_va=[]; y_va=[]; g_va=[]\n",
        "    cnt=0\n",
        "    def make_feats(cart, row, cand):\n",
        "        f0 = pop.get(cand, 0)\n",
        "        f1 = sum(maps[0].get(x,{}).get(cand,0) for x in cart)  # global co-vis sum\n",
        "        f2 = len(cart)\n",
        "        f3 = int(cand in global_top[:50])\n",
        "        return [f0, f1, f2, f3]\n",
        "    for row in row_iter(order_path):\n",
        "        items = extract_items(row.get(\"ORDERS\",\"\"))\n",
        "        if len(items) < 2: continue\n",
        "        target = items[-1]; cart = items[:-1]\n",
        "        scores = blended_scores(cart, row, *maps, pop)\n",
        "        cands = [c for c,_ in scores.most_common(cand_top) if c not in cart]\n",
        "        if not cands: continue\n",
        "        X = [make_feats(cart, row, c) for c in cands]\n",
        "        Y = [1 if c==target else 0 for c in cands]\n",
        "        if cnt < n_orders_train:\n",
        "            X_tr.extend(X); y_tr.extend(Y); g_tr.append(len(X))\n",
        "        elif cnt < n_orders_train+n_orders_valid:\n",
        "            X_va.extend(X); y_va.extend(Y); g_va.append(len(X))\n",
        "        else:\n",
        "            break\n",
        "        cnt+=1\n",
        "        if cnt % 200_000 == 0:\n",
        "            print(f\"Built {cnt:,} training orders...\")\n",
        "    ranker = LGBMRanker(\n",
        "        n_estimators=200, learning_rate=0.08, max_depth=-1,\n",
        "        subsample=0.85, colsample_bytree=0.85, objective=\"lambdarank\",\n",
        "        random_state=42\n",
        "    )\n",
        "    ranker.fit(np.array(X_tr, dtype=np.float32), np.array(y_tr), group=g_tr,\n",
        "               eval_set=[(np.array(X_va, dtype=np.float32), np.array(y_va))], eval_group=[g_va])\n",
        "    return ranker\n",
        "\n",
        "def predict_with_reranker(cart, row, maps, global_top, pop, ranker, k=3, cand_top=120):\n",
        "    scores = blended_scores(cart, row, *maps, pop)\n",
        "    cands = [c for c,_ in scores.most_common(cand_top) if c not in cart]\n",
        "    if not cands:\n",
        "        return (global_top[:k] + [\"\",\"\",\"\"])[:k]\n",
        "    # same 4 lightweight features as training\n",
        "    def make_feats(cart, row, cand):\n",
        "        f0 = pop.get(cand, 0)\n",
        "        f1 = sum(maps[0].get(x,{}).get(cand,0) for x in cart)\n",
        "        f2 = len(cart)\n",
        "        f3 = int(cand in global_top[:50])\n",
        "        return [f0, f1, f2, f3]\n",
        "    X = np.array([make_feats(cart, row, c) for c in cands], dtype=np.float32)\n",
        "    preds = ranker.predict(X)\n",
        "    order = np.argsort(-preds)\n",
        "    ranked = [cands[i] for i in order[:k]]\n",
        "    return (ranked + [\"\",\"\",\"\"])[:k]\n",
        "\n",
        "# ---------------------------------\n",
        "# 8) Main\n",
        "# ---------------------------------\n",
        "def main(order_path=ORDER_PATH, test_path=TEST_PATH, out_dir=OUT_DIR,\n",
        "         use_reranker=False, do_eval=True, do_temporal=True):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    paths = out_paths(out_dir)\n",
        "\n",
        "    # Build & normalize\n",
        "    (cg, by_ch, by_sc, by_oc, by_st, by_ct, pop) = build_covis_maps(order_path)\n",
        "    cg      = normalize_covis(cg, pop, power=0.5)\n",
        "    by_ch   = norm_nested(by_ch, pop)\n",
        "    by_sc   = norm_nested(by_sc, pop)\n",
        "    by_oc   = norm_nested(by_oc, pop)\n",
        "    by_st   = norm_nested(by_st, pop)\n",
        "    by_ct   = norm_nested(by_ct, pop)\n",
        "    maps    = (cg, by_ch, by_sc, by_oc, by_st, by_ct)\n",
        "    global_top = [it for it,_ in pop.most_common(2000)]\n",
        "\n",
        "    # Submissions\n",
        "    write_excel(test_path, paths[\"OUT_MAX\"],   maps, global_top, pop, use_mmr=False, reasons_csv=paths[\"REASONS\"])\n",
        "    write_excel(test_path, paths[\"OUT_TUNED\"], maps, global_top, pop, use_mmr=True)\n",
        "\n",
        "    # Optional reranker (if you want a 3rd sheet for your deck)\n",
        "    if use_reranker:\n",
        "        print(\"Training LightGBM reranker (optional)...\")\n",
        "        ranker = train_reranker(order_path, maps, global_top, pop)\n",
        "        # produce an LTR file as well (for storytelling)\n",
        "        df = pd.read_csv(test_path, dtype=str, keep_default_na=False)\n",
        "        item_cols = [c for c in df.columns if c.upper().startswith(\"ITEM\")]\n",
        "        p1,p2,p3=[],[],[]\n",
        "        for _, r in tqdm(df.iterrows(), total=len(df), desc=\"Predict LTR\"):\n",
        "            cart = [normalize_text(r[c]) for c in item_cols if r.get(c,\"\")]\n",
        "            recs = predict_with_reranker(cart, r, maps, global_top, pop, ranker, k=3)\n",
        "            p1.append(recs[0]); p2.append(recs[1]); p3.append(recs[2])\n",
        "        out = df.copy()\n",
        "        out[\"RECOMMENDATION 1\"]=p1; out[\"RECOMMENDATION 2\"]=p2; out[\"RECOMMENDATION 3\"]=p3\n",
        "        out.to_excel(os.path.join(out_dir, \"Recommendation_Output_LTR.xlsx\"), index=False)\n",
        "        print(\"Wrote:\", os.path.join(out_dir, \"Recommendation_Output_LTR.xlsx\"))\n",
        "\n",
        "    # Evaluations for deck (judge-friendly)\n",
        "    metrics = {}\n",
        "    if do_eval:\n",
        "        print(\"Evaluating (strict LOO sample)...\")\n",
        "        m_strict = eval_strict_loo(order_path, maps, global_top, pop, n_eval=5000)\n",
        "        print(\"Strict LOO metrics:\", m_strict); metrics[\"strict_looo\"] = m_strict\n",
        "    if do_temporal:\n",
        "        print(\"Evaluating (temporal holdout sample)...\")\n",
        "        m_temp = temporal_eval(order_path, frac_train=0.90, max_eval=8000)\n",
        "        print(\"Temporal metrics:\", m_temp); metrics[\"temporal\"] = m_temp\n",
        "\n",
        "    # Save metrics\n",
        "    if metrics:\n",
        "        with open(paths[\"MET_JSON\"], \"w\") as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        with open(paths[\"MET_CSV\"], \"w\") as f:\n",
        "            f.write(\"Eval,Num,Recall@1,Recall@2,Recall@3,MAP@3,NDCG@3\\n\")\n",
        "            if \"strict_looo\" in metrics:\n",
        "                m=metrics[\"strict_looo\"]\n",
        "                f.write(f\"Strict LOO,{m['num_eval_orders']},{m['Recall@1']},{m['Recall@2']},{m['Recall@3']},{m['MAP@3']},{m['NDCG@3']}\\n\")\n",
        "            if \"temporal\" in metrics:\n",
        "                m=metrics[\"temporal\"]\n",
        "                f.write(f\"Temporal,{m['num_eval_orders']},{m['Recall@1']},{m['Recall@2']},{m['Recall@3']},{m['MAP@3']},{m['NDCG@3']}\\n\")\n",
        "        print(\"Saved metrics:\", paths[\"MET_JSON\"], \"and\", paths[\"MET_CSV\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--order_path\", type=str, default=ORDER_PATH)\n",
        "    parser.add_argument(\"--test_path\",  type=str, default=TEST_PATH)\n",
        "    parser.add_argument(\"--out_dir\",    type=str, default=OUT_DIR)\n",
        "    parser.add_argument(\"--use_reranker\", type=lambda x: str(x).lower() in {\"1\",\"true\",\"yes\",\"y\"}, default=False)\n",
        "    parser.add_argument(\"--do_eval\",    type=lambda x: str(x).lower() in {\"1\",\"true\",\"yes\",\"y\"}, default=True)\n",
        "    parser.add_argument(\"--do_temporal\",type=lambda x: str(x).lower() in {\"1\",\"true\",\"yes\",\"y\"}, default=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(order_path=args.order_path,\n",
        "         test_path=args.test_path,\n",
        "         out_dir=args.out_dir,\n",
        "         use_reranker=args.use_reranker,\n",
        "         do_eval=args.do_eval,\n",
        "         do_temporal=args.do_temporal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "9P7SP6xTZ1-i",
        "outputId": "edce5f8f-0001-468e-eb3d-b3934acd21f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--order_path ORDER_PATH]\n",
            "                                [--test_path TEST_PATH] [--out_dir OUT_DIR]\n",
            "                                [--use_reranker USE_RERANKER]\n",
            "                                [--do_eval DO_EVAL]\n",
            "                                [--do_temporal DO_TEMPORAL]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-d074196e-a292-47cd-af92-36521897338d.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYPIw_7illGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CLI / entrypoint (put this at the very end of your script) ---\n",
        "import argparse\n",
        "\n",
        "def build_parser():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--order_path\", default=\"/content/order_data.csv\")\n",
        "    p.add_argument(\"--test_path\",  default=\"/content/test_data_question.csv\")\n",
        "    p.add_argument(\"--out_dir\",    default=\"/content\")\n",
        "    p.add_argument(\"--use_reranker\", type=lambda s: s.lower() in {\"1\",\"true\",\"yes\"}, default=False)\n",
        "    p.add_argument(\"--do_eval\",      type=lambda s: s.lower() in {\"1\",\"true\",\"yes\"}, default=True)\n",
        "    p.add_argument(\"--do_temporal\",  type=lambda s: s.lower() in {\"1\",\"true\",\"yes\"}, default=False)\n",
        "    return p\n",
        "\n",
        "def main_cli():\n",
        "    parser = build_parser()\n",
        "    # ðŸ‘‡ swallow notebook/kernel flags like \"-f /path/kernel.json\"\n",
        "    args, _unknown = parser.parse_known_args()\n",
        "    # call your existing main using args\n",
        "    # main(order_path=args.order_path, test_path=args.test_path, out_dir=args.out_dir,\n",
        "    #      use_reranker=args.use_reranker, do_eval=args.do_eval, do_temporal=args.do_temporal)\n",
        "    main(\n",
        "        order_path=args.order_path,\n",
        "        test_path=args.test_path,\n",
        "        out_dir=args.out_dir,\n",
        "        use_reranker=args.use_reranker,\n",
        "        do_eval=args.do_eval,\n",
        "        do_temporal=args.do_temporal\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_cli()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRYTk2bxd8Gm",
        "outputId": "178f006f-6d4e-4890-a147-dd7d6ea2671d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building co-vis maps: 200557it [00:54, 3864.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 200,000 rows...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building co-vis maps: 400365it [01:51, 3444.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 400,000 rows...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building co-vis maps: 519662it [02:27, 3531.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows parsed: 519,662 | Orders with items: 519,662 | Unique items: 3463\n",
            "Build time: 2.45 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict Max: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 6038.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/Recommendation_Output_MAX.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predict MMR: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 436.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/Recommendation_Output_TUNED.xlsx\n",
            "Evaluating (strict LOO sample)...\n",
            "Strict LOO metrics: {'num_eval_orders': 5000, 'Recall@1': 0.8724, 'Recall@2': 0.8738, 'Recall@3': 0.874, 'MAP@3': 0.8732, 'NDCG@3': 0.8734}\n",
            "Saved metrics: /content/metrics.json and /content/metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "out_dir = \"/content\"  # change if needed\n",
        "max_path   = f\"{out_dir}/Recommendation_Output_MAX.xlsx\"\n",
        "tuned_path = f\"{out_dir}/Recommendation_Output_TUNED.xlsx\"\n",
        "\n",
        "# 1) Columns exist\n",
        "req_cols = {\"CUSTOMER_ID\",\"ORDER_ID\",\"RECOMMENDATION 1\",\"RECOMMENDATION 2\",\"RECOMMENDATION 3\"}\n",
        "df = pd.read_excel(max_path)\n",
        "assert req_cols.issubset(df.columns), f\"Missing columns: {req_cols - set(df.columns)}\"\n",
        "\n",
        "# 2) Row count matches test set\n",
        "test_df = pd.read_csv(\"/content/test_data_question.csv\", dtype=str, keep_default_na=False)\n",
        "assert len(df)==len(test_df), f\"Row mismatch: {len(df)} vs test {len(test_df)}\"\n",
        "\n",
        "# 3) No blanks in recs\n",
        "assert df[[\"RECOMMENDATION 1\",\"RECOMMENDATION 2\",\"RECOMMENDATION 3\"]].notna().all().all()\n",
        "\n",
        "# 4) No duplicate recs per row\n",
        "dups = (df[\"RECOMMENDATION 1\"]==df[\"RECOMMENDATION 2\"]) | \\\n",
        "       (df[\"RECOMMENDATION 1\"]==df[\"RECOMMENDATION 3\"]) | \\\n",
        "       (df[\"RECOMMENDATION 2\"]==df[\"RECOMMENDATION 3\"])\n",
        "assert dups.sum()==0, f\"Found {dups.sum()} rows with duplicate recommendations\"\n",
        "\n",
        "print(\"âœ… Output sheet passes all checks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvNOxP34lmJB",
        "outputId": "fbab2fec-1c2c-4097-f2e9-a65a187044cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Output sheet passes all checks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CONFIG (edit if needed) ====\n",
        "ORDER_PATH = \"/content/order_data.csv\"\n",
        "TEST_PATH  = \"/content/test_data_question.csv\"\n",
        "OUT_DIR    = \"/content\"\n",
        "\n",
        "import os, re, json, math, datetime, random\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "random.seed(42); np.random.seed(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers (robust row reader + item parsing)\n",
        "# -----------------------------\n",
        "EXPECTED_COLS = (\"CUSTOMER_ID\",\"ORDER_ID\",\n",
        "                 \"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\n",
        "                 \"ORDER_OCCASION_NAME\",\"STORE_NUMBER\",\n",
        "                 \"CUSTOMER_TYPE\",\"ORDERS\")\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def split_items(s: str):\n",
        "    if s is None: return []\n",
        "    t = str(s).strip().strip('\"').strip(\"'\")\n",
        "    if not t: return []\n",
        "    if t.startswith(\"[\") and t.endswith(\"]\"):\n",
        "        try:\n",
        "            arr = json.loads(t)\n",
        "            return [normalize_text(x) for x in arr if str(x).strip()]\n",
        "        except Exception:\n",
        "            pass\n",
        "    for sep in [\"|\",\";\",\"||\",\"\\t\",\",\"]:\n",
        "        if sep in t:\n",
        "            return [normalize_text(x) for x in t.split(sep) if x.strip()]\n",
        "    return [normalize_text(x) for x in re.split(r\"[|;,]\\s*|\\s{2,}\", t) if x]\n",
        "\n",
        "def extract_items(order_text) -> list:\n",
        "    items = [x for x in split_items(order_text) if x]\n",
        "    # unique but keep first occurrence order\n",
        "    seen=set(); out=[]\n",
        "    for it in items:\n",
        "        if it not in seen:\n",
        "            seen.add(it); out.append(it)\n",
        "    return out\n",
        "\n",
        "def row_iter(path, expected_cols=EXPECTED_COLS):\n",
        "    exp_n = len(expected_cols)\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        _ = f.readline()  # header\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\",\", exp_n - 1)\n",
        "            if len(parts) < exp_n:\n",
        "                continue\n",
        "            yield dict(zip(expected_cols, parts[:exp_n]))\n",
        "\n",
        "# -----------------------------\n",
        "# If you already have these maps from earlier cells, we reuse them.\n",
        "# Otherwise we build minimally here.\n",
        "# -----------------------------\n",
        "need_build = False\n",
        "for var in [\"covis_global\",\"covis_by_channel\",\"covis_by_subch\",\n",
        "            \"covis_by_occ\",\"covis_by_store\",\"covis_by_custtype\",\n",
        "            \"pop_global\",\"global_top\",\"cart_size_hist\"]:\n",
        "    if var not in globals():\n",
        "        need_build = True\n",
        "        break\n",
        "\n",
        "if need_build:\n",
        "    covis_global      = defaultdict(Counter)\n",
        "    covis_by_channel  = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_subch    = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_occ      = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_store    = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_custtype = defaultdict(lambda: defaultdict(Counter))\n",
        "    pop_global        = Counter()\n",
        "    cart_size_hist    = Counter()\n",
        "\n",
        "    print(\"Building co-vis maps (quick build)...\")\n",
        "    for r in tqdm(row_iter(ORDER_PATH), unit=\"it\"):\n",
        "        items = extract_items(r.get(\"ORDERS\",\"\"))\n",
        "        cart_size_hist[len(items)] += 1\n",
        "        if not items:\n",
        "            continue\n",
        "        for it in set(items): pop_global[it]+=1\n",
        "        uniq = list(set(items))\n",
        "        n=len(uniq)\n",
        "        if n>1:\n",
        "            ch=r.get(\"ORDER_CHANNEL_NAME\",\"\"); sc=r.get(\"ORDER_SUBCHANNEL_NAME\",\"\")\n",
        "            oc=r.get(\"ORDER_OCCASION_NAME\",\"\"); st=r.get(\"STORE_NUMBER\",\"\")\n",
        "            ct=r.get(\"CUSTOMER_TYPE\",\"\")\n",
        "            for i in range(n):\n",
        "                a=uniq[i]\n",
        "                for j in range(i+1,n):\n",
        "                    b=uniq[j]\n",
        "                    covis_global[a][b]+=1; covis_global[b][a]+=1\n",
        "                    covis_by_channel[ch][a][b]+=1;  covis_by_channel[ch][b][a]+=1\n",
        "                    covis_by_subch[sc][a][b]+=1;    covis_by_subch[sc][b][a]+=1\n",
        "                    covis_by_occ[oc][a][b]+=1;      covis_by_occ[oc][b][a]+=1\n",
        "                    covis_by_store[st][a][b]+=1;    covis_by_store[st][b][a]+=1\n",
        "                    covis_by_custtype[ct][a][b]+=1; covis_by_custtype[ct][b][a]+=1\n",
        "    global_top = [it for it,_ in pop_global.most_common(2000)]\n",
        "else:\n",
        "    print(\"Reusing previously built maps.\")\n",
        "\n",
        "# Normalize (keeps top neighbors; improves stability)\n",
        "def normalize_covis(cmap: dict, pop: Counter, power=0.5, keep_top=500):\n",
        "    out = {}\n",
        "    for a, neigh in cmap.items():\n",
        "        pa = max(pop[a], 1)\n",
        "        denom_a = pa**power\n",
        "        d = {}\n",
        "        for b, c in neigh.items():\n",
        "            pb = max(pop[b], 1)\n",
        "            d[b] = float(c) / (denom_a * (pb**power))\n",
        "        out[a] = dict(Counter(d).most_common(keep_top))\n",
        "    return out\n",
        "\n",
        "def norm_nested(ctx_map, pop):\n",
        "    out = {}\n",
        "    for key, cmap in ctx_map.items():\n",
        "        out[key] = normalize_covis(cmap, pop, power=0.5)\n",
        "    return out\n",
        "\n",
        "covis_global_n      = normalize_covis(covis_global, pop_global, power=0.5)\n",
        "covis_by_channel_n  = norm_nested(covis_by_channel,  pop_global)\n",
        "covis_by_subch_n    = norm_nested(covis_by_subch,    pop_global)\n",
        "covis_by_occ_n      = norm_nested(covis_by_occ,      pop_global)\n",
        "covis_by_store_n    = norm_nested(covis_by_store,    pop_global)\n",
        "covis_by_custtype_n = norm_nested(covis_by_custtype, pop_global)\n",
        "\n",
        "# Tuned weights from your best run\n",
        "W = (0.51, 0.24, 0.05, 0.11, 0.17, 0.10)   # (global, channel, subchannel, occasion, store, customer_type)\n",
        "LAM = 0.93\n",
        "BACKOFF_ALPHA = 0.15\n",
        "CAND_POOL = 120\n",
        "\n",
        "def blended_scores(cart, row):\n",
        "    scores = Counter()\n",
        "    def add_from(map_, weight):\n",
        "        if weight <= 0 or not map_: return\n",
        "        for it in cart:\n",
        "            for c, v in map_.get(it, {}).items():\n",
        "                scores[c] += weight * v\n",
        "\n",
        "    ch = str(row.get(\"ORDER_CHANNEL_NAME\",\"\"))\n",
        "    sc = str(row.get(\"ORDER_SUBCHANNEL_NAME\",\"\"))\n",
        "    oc = str(row.get(\"ORDER_OCCASION_NAME\",\"\"))\n",
        "    st = str(row.get(\"STORE_NUMBER\",\"\"))\n",
        "    ct = str(row.get(\"CUSTOMER_TYPE\",\"\"))\n",
        "\n",
        "    add_from(covis_global_n, W[0])\n",
        "    if ch in covis_by_channel_n:  add_from(covis_by_channel_n[ch],  W[1])\n",
        "    if sc in covis_by_subch_n:    add_from(covis_by_subch_n[sc],    W[2])\n",
        "    if oc in covis_by_occ_n:      add_from(covis_by_occ_n[oc],      W[3])\n",
        "    if st in covis_by_store_n:    add_from(covis_by_store_n[st],    W[4])\n",
        "    if ct in covis_by_custtype_n: add_from(covis_by_custtype_n[ct], W[5])\n",
        "\n",
        "    for it in cart: scores.pop(it, None)\n",
        "\n",
        "    # backoff for sparse contexts\n",
        "    if len(scores) < 5:\n",
        "        for it in cart:\n",
        "            for c, v in covis_global_n.get(it, {}).items():\n",
        "                scores[c] += BACKOFF_ALPHA * v\n",
        "        for it in cart: scores.pop(it, None)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def recommend_simple(cart, row, k=3):\n",
        "    scores = blended_scores(cart, row)\n",
        "    cands = [c for c,_ in scores.most_common(800) if c not in cart][:CAND_POOL]\n",
        "    # backfill popular if needed\n",
        "    for g in global_top:\n",
        "        if len(cands) >= CAND_POOL: break\n",
        "        if g not in cart and g not in cands: cands.append(g)\n",
        "    final = cands[:k]\n",
        "    return (final + [\"\",\"\",\"\"])[:k], scores\n",
        "\n",
        "# -----------------------------\n",
        "# Strict LOO + Temporal metrics\n",
        "# -----------------------------\n",
        "def eval_strict_loo(n_eval=5000):\n",
        "    hits1=hits2=hits3=map3=ndcg3=0.0; seen=0\n",
        "    for r in row_iter(ORDER_PATH):\n",
        "        items = extract_items(r.get(\"ORDERS\",\"\"))\n",
        "        uniq = list(dict.fromkeys(items))\n",
        "        if len(uniq) < 2: continue\n",
        "        tgt = uniq[-1]\n",
        "        cart = [x for x in uniq if x!=tgt]\n",
        "        if not cart: continue\n",
        "        preds,_ = recommend_simple(cart, r, k=3)\n",
        "        if preds[0]==tgt: hits1+=1\n",
        "        if tgt in preds[:2]: hits2+=1\n",
        "        if tgt in preds[:3]: hits3+=1\n",
        "        if tgt in preds[:3]:\n",
        "            rank = preds.index(tgt)+1\n",
        "            map3 += 1.0/rank\n",
        "            ndcg3+= 1.0/math.log2(rank+1)\n",
        "        seen += 1\n",
        "        if seen>=n_eval: break\n",
        "    return {\n",
        "        \"num_eval_orders\": seen,\n",
        "        \"Recall@1\": round(hits1/seen,4),\n",
        "        \"Recall@2\": round(hits2/seen,4),\n",
        "        \"Recall@3\": round(hits3/seen,4),\n",
        "        \"MAP@3\": round(map3/seen,4),\n",
        "        \"NDCG@3\": round(ndcg3/seen,4)\n",
        "    }\n",
        "\n",
        "def count_rows_fast(path):  # for temporal split location\n",
        "    return max(0, sum(1 for _ in open(path, \"r\", encoding=\"utf-8\", errors=\"replace\")) - 1)\n",
        "\n",
        "def eval_temporal_tail(tail_frac=0.1, max_eval=8000):\n",
        "    total = count_rows_fast(ORDER_PATH)\n",
        "    train_cut = int(total*(1-tail_frac))\n",
        "    hits1=hits2=hits3=map3=ndcg3=0.0; seen=0; i=0\n",
        "    for r in row_iter(ORDER_PATH):\n",
        "        i+=1\n",
        "        if i<=train_cut:\n",
        "            continue\n",
        "        items = extract_items(r.get(\"ORDERS\",\"\"))\n",
        "        uniq = list(dict.fromkeys(items))\n",
        "        if len(uniq) < 2: continue\n",
        "        tgt = uniq[-1]\n",
        "        cart = [x for x in uniq if x!=tgt]\n",
        "        if not cart: continue\n",
        "        preds,_ = recommend_simple(cart, r, k=3)\n",
        "        if preds[0]==tgt: hits1+=1\n",
        "        if tgt in preds[:2]: hits2+=1\n",
        "        if tgt in preds[:3]: hits3+=1\n",
        "        if tgt in preds[:3]:\n",
        "            rank = preds.index(tgt)+1\n",
        "            map3 += 1.0/rank\n",
        "            ndcg3+= 1.0/math.log2(rank+1)\n",
        "        seen += 1\n",
        "        if seen>=max_eval: break\n",
        "    return {\n",
        "        \"num_eval_orders\": seen,\n",
        "        \"Recall@1\": round(hits1/seen,4),\n",
        "        \"Recall@2\": round(hits2/seen,4),\n",
        "        \"Recall@3\": round(hits3/seen,4),\n",
        "        \"MAP@3\": round(map3/seen,4),\n",
        "        \"NDCG@3\": round(ndcg3/seen,4)\n",
        "    }\n",
        "\n",
        "strict_m = eval_strict_loo(n_eval=5000)\n",
        "temp_m   = eval_temporal_tail(tail_frac=0.1, max_eval=8000)\n",
        "print(\"Strict LOO:\", strict_m)\n",
        "print(\"Temporal:  \", temp_m)\n",
        "\n",
        "# Save metrics\n",
        "with open(os.path.join(OUT_DIR, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump({\"strict_looo\": strict_m, \"temporal\": temp_m}, f, indent=2)\n",
        "with open(os.path.join(OUT_DIR, \"metrics.csv\"), \"w\") as f:\n",
        "    f.write(\"Eval,Num,Recall@1,Recall@2,Recall@3,MAP@3,NDCG@3\\n\")\n",
        "    f.write(\"Strict LOO,{num},{r1},{r2},{r3},{map3},{ndcg3}\\n\".format(\n",
        "        num=strict_m[\"num_eval_orders\"], r1=strict_m[\"Recall@1\"], r2=strict_m[\"Recall@2\"],\n",
        "        r3=strict_m[\"Recall@3\"], map3=strict_m[\"MAP@3\"], ndcg3=strict_m[\"NDCG@3\"]))\n",
        "    f.write(\"Temporal,{num},{r1},{r2},{r3},{map3},{ndcg3}\\n\".format(\n",
        "        num=temp_m[\"num_eval_orders\"], r1=temp_m[\"Recall@1\"], r2=temp_m[\"Recall@2\"],\n",
        "        r3=temp_m[\"Recall@3\"], map3=temp_m[\"MAP@3\"], ndcg3=temp_m[\"NDCG@3\"]))\n",
        "\n",
        "# -----------------------------\n",
        "# Charts: long tail, top-20, cart size, heatmap, recall, calibration, lift\n",
        "# -----------------------------\n",
        "# Long tail\n",
        "freqs = [v for _,v in pop_global.most_common()]\n",
        "plt.figure(figsize=(7,4.5)); plt.plot(range(1,len(freqs)+1), freqs)\n",
        "plt.xlabel(\"Items sorted by popularity rank\"); plt.ylabel(\"Orders count\")\n",
        "plt.title(\"Item Popularity Long Tail\"); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"long_tail.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Top-20\n",
        "top20 = pop_global.most_common(20)\n",
        "plt.figure(figsize=(8,5)); plt.bar([k for k,_ in top20], [v for _,v in top20])\n",
        "plt.xticks(rotation=60, ha=\"right\"); plt.ylabel(\"Orders count\")\n",
        "plt.title(\"Top 20 Items\"); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"top20.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Cart size histogram\n",
        "xs = sorted(dict(cart_size_hist).items())\n",
        "plt.figure(figsize=(7,4.5)); plt.bar([k for k,_ in xs], [v for _,v in xs])\n",
        "plt.xlabel(\"Cart size\"); plt.ylabel(\"Orders\")\n",
        "plt.title(\"Cart Size Distribution\"); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"cart_size_hist.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Co-vis heatmap (Top-20, normalized)\n",
        "import numpy as np\n",
        "top20_items = [k for k,_ in top20][:20]\n",
        "M = np.zeros((len(top20_items), len(top20_items)), dtype=float)\n",
        "for i,a in enumerate(top20_items):\n",
        "    neigh = covis_global.get(a, {})\n",
        "    for j,b in enumerate(top20_items):\n",
        "        if a==b: continue\n",
        "        M[i,j] = neigh.get(b, 0.0)\n",
        "if M.max() > 0:\n",
        "    M = M / (M.max()+1e-9)\n",
        "plt.figure(figsize=(7,6)); plt.imshow(M, aspect=\"auto\")\n",
        "plt.xticks(range(len(top20_items)), top20_items, rotation=60, ha=\"right\")\n",
        "plt.yticks(range(len(top20_items)), top20_items)\n",
        "plt.title(\"Normalized Co-Visitation Heatmap (Top-20)\"); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"covis_heatmap.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Recall@k bar chart (Strict LOO vs Temporal)\n",
        "labels = [\"R@1\",\"R@2\",\"R@3\"]\n",
        "vals_m = [strict_m[\"Recall@1\"], strict_m[\"Recall@2\"], strict_m[\"Recall@3\"]]\n",
        "vals_t = [temp_m[\"Recall@1\"],   temp_m[\"Recall@2\"],   temp_m[\"Recall@3\"]]\n",
        "x = np.arange(3)\n",
        "plt.figure(figsize=(6.5,4.5))\n",
        "plt.bar(x-0.15, vals_m, width=0.3, label=\"Strict LOO\")\n",
        "plt.bar(x+0.15, vals_t, width=0.3, label=\"Temporal\")\n",
        "plt.xticks(x, labels); plt.ylim(0,1); plt.legend()\n",
        "plt.title(\"Recall@k â€” Strict LOO vs Temporal\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"recall_chart.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Calibration (top-1 confidence proxy = softmax of top-3 raw scores)\n",
        "def softmax(x):\n",
        "    x = np.array(x, dtype=np.float32)\n",
        "    x = x - x.max()\n",
        "    e = np.exp(x)\n",
        "    s = e.sum() + 1e-12\n",
        "    return (e / s).tolist()\n",
        "\n",
        "confs=[]; hits=[]\n",
        "i=0\n",
        "for r in row_iter(ORDER_PATH):\n",
        "    items = extract_items(r.get(\"ORDERS\",\"\"))\n",
        "    uniq = list(dict.fromkeys(items))\n",
        "    if len(uniq)<2: continue\n",
        "    tgt = uniq[-1]; cart = [x for x in uniq if x!=tgt]\n",
        "    if not cart: continue\n",
        "    preds, scores = recommend_simple(cart, r, k=3)\n",
        "    raw = [scores.get(p, 1.0/(j+1)) for j,p in enumerate(preds[:3])]\n",
        "    c = softmax(raw)[0] if raw else 0.33\n",
        "    confs.append(c)\n",
        "    hits.append(1.0 if (len(preds)>0 and preds[0]==tgt) else 0.0)\n",
        "    i+=1\n",
        "    if i>=4000: break\n",
        "\n",
        "confs = np.array(confs); hits = np.array(hits)\n",
        "dec = np.minimum((confs*10).astype(int), 9)\n",
        "xb, yb = [], []\n",
        "for d in range(10):\n",
        "    m = (dec==d)\n",
        "    xb.append(d/10.0); yb.append(float(hits[m].mean()) if m.sum()>0 else 0.0)\n",
        "\n",
        "plt.figure(figsize=(6.5,4.5))\n",
        "plt.plot(xb, yb)\n",
        "plt.plot([0,0.9],[xb[0],xb[-1]], linestyle=\"--\")\n",
        "plt.xlabel(\"Confidence decile (lower bound)\"); plt.ylabel(\"Empirical Hit-Rate (Top-1)\")\n",
        "plt.title(\"Confidence Calibration (Top-1)\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"confidence_calibration.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Lift curve\n",
        "order = np.argsort(-confs)\n",
        "cum_hits = np.cumsum(hits[order])\n",
        "pct = np.arange(1, len(confs)+1) / len(confs)\n",
        "lift = cum_hits / (np.arange(1,len(confs)+1) * hits.mean() + 1e-9)\n",
        "plt.figure(figsize=(6.5,4.5)); plt.plot(pct, lift)\n",
        "plt.xlabel(\"Fraction of recommendations shown\"); plt.ylabel(\"Lift vs random\")\n",
        "plt.title(\"Lift Curve (Top-1 by confidence)\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"lift_curve.png\"), dpi=160); plt.close()\n",
        "\n",
        "# Context weights (if you don't have a model FI)\n",
        "plt.figure(figsize=(6.5,4.5))\n",
        "plt.bar(range(len(W)), list(W))\n",
        "plt.xticks(range(len(W)), [\"Global\",\"Channel\",\"Subchannel\",\"Occasion\",\"Store\",\"CustomerType\"])\n",
        "plt.ylim(0, max(W)+0.1)\n",
        "plt.title(\"Context Contribution Weights (Tuned)\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"context_weights.png\"), dpi=160); plt.close()\n",
        "\n",
        "print(\"Saved graphs + metrics in:\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qakwGhMqmyCG",
        "outputId": "29c7e094-562e-490d-b3eb-59dc45c72f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building co-vis maps (quick build)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1070135it [06:10, 2889.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strict LOO: {'num_eval_orders': 5000, 'Recall@1': 0.8716, 'Recall@2': 0.8738, 'Recall@3': 0.874, 'MAP@3': 0.8728, 'NDCG@3': 0.8731}\n",
            "Temporal:   {'num_eval_orders': 8000, 'Recall@1': 0.8498, 'Recall@2': 0.8515, 'Recall@3': 0.8525, 'MAP@3': 0.851, 'NDCG@3': 0.8514}\n",
            "Saved graphs + metrics in: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Micro-tuner for context-blended co-vis recommender ====\n",
        "# Tries small grids for (Channel, Store, Occasion weights), CAND_POOL, and BACKOFF_ALPHA.\n",
        "# Produces: tuning_report.csv, tuning_plot.png, best_config.json\n",
        "# Optional: rewrites Recommendation_Output_MAX.xlsx using the best config.\n",
        "\n",
        "import os, re, csv, json, math, random, time\n",
        "from collections import defaultdict, Counter\n",
        "from statistics import mean\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_OK = True\n",
        "except Exception:\n",
        "    PANDAS_OK = False\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------- Paths (edit if needed) ----------\n",
        "ORDER_PATH = os.getenv(\"ORDER_PATH\", \"/content/order_data.csv\")\n",
        "TEST_PATH  = os.getenv(\"TEST_PATH\",  \"/content/test_data_question.csv\")\n",
        "OUT_DIR    = os.getenv(\"OUT_DIR\",    \"/content\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------- Quick controls ----------\n",
        "MAX_ROWS_BUILD   = int(os.getenv(\"MAX_ROWS_BUILD\", \"300000\"))  # subset for fast tuning\n",
        "N_EVAL           = int(os.getenv(\"N_EVAL\", \"3000\"))            # strict-LOO eval sample\n",
        "WRITE_FINAL_XLSX = os.getenv(\"WRITE_FINAL_XLSX\", \"false\").lower() in {\"1\",\"true\",\"yes\"}\n",
        "\n",
        "# --------- Robust line reader (preserves commas in last column) ----------\n",
        "EXPECTED = (\"CUSTOMER_ID\",\"ORDER_ID\",\"ORDER_CHANNEL_NAME\",\"ORDER_SUBCHANNEL_NAME\",\n",
        "            \"ORDER_OCCASION_NAME\",\"STORE_NUMBER\",\"CUSTOMER_TYPE\",\"ORDERS\")\n",
        "EXP_N = len(EXPECTED)\n",
        "\n",
        "def row_iter(path, max_rows=None):\n",
        "    n = 0\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        _ = f.readline()  # header (ignored)\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\",\", EXP_N - 1)\n",
        "            if len(parts) < EXP_N:\n",
        "                continue\n",
        "            row = dict(zip(EXPECTED, parts[:EXP_N]))\n",
        "            yield row\n",
        "            n += 1\n",
        "            if max_rows and n >= max_rows:\n",
        "                break\n",
        "\n",
        "def extract_items(s):\n",
        "    if s is None: return []\n",
        "    s = str(s).strip().strip('\"').strip(\"'\")\n",
        "    if not s: return []\n",
        "    parts = re.split(r'[|,;]\\s*', s)\n",
        "    out=[]; seen=set()\n",
        "    for p in parts:\n",
        "        p=p.strip()\n",
        "        if not p or p.lower() in {\"na\",\"none\",\"null\",\"nan\"}:\n",
        "            continue\n",
        "        if p not in seen:\n",
        "            seen.add(p); out.append(p)\n",
        "    return out\n",
        "\n",
        "# --------- Build co-vis maps (subset for speed) ----------\n",
        "def build_covis(limit_rows=None):\n",
        "    covis_global      = defaultdict(Counter)\n",
        "    covis_by_channel  = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_subch    = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_occ      = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_store    = defaultdict(lambda: defaultdict(Counter))\n",
        "    covis_by_custtype = defaultdict(lambda: defaultdict(Counter))\n",
        "    pop = Counter()\n",
        "\n",
        "    t0=time.time(); rows=0; orders=0\n",
        "    for r in row_iter(ORDER_PATH, max_rows=limit_rows):\n",
        "        items = extract_items(r.get(\"ORDERS\",\"\"))\n",
        "        if not items:\n",
        "            rows += 1;\n",
        "            continue\n",
        "        orders += 1; rows += 1\n",
        "        uniq = list(dict.fromkeys(items))\n",
        "        for it in uniq: pop[it]+=1\n",
        "        L=len(uniq)\n",
        "        if L>1:\n",
        "            ch=r.get(\"ORDER_CHANNEL_NAME\",\"\"); sc=r.get(\"ORDER_SUBCHANNEL_NAME\",\"\")\n",
        "            oc=r.get(\"ORDER_OCCASION_NAME\",\"\"); st=r.get(\"STORE_NUMBER\",\"\")\n",
        "            ct=r.get(\"CUSTOMER_TYPE\",\"\")\n",
        "            for i in range(L):\n",
        "                a=uniq[i]\n",
        "                for j in range(i+1, L):\n",
        "                    b=uniq[j]\n",
        "                    covis_global[a][b]+=1; covis_global[b][a]+=1\n",
        "                    covis_by_channel[ch][a][b]+=1;  covis_by_channel[ch][b][a]+=1\n",
        "                    covis_by_subch[sc][a][b]+=1;    covis_by_subch[sc][b][a]+=1\n",
        "                    covis_by_occ[oc][a][b]+=1;      covis_by_occ[oc][b][a]+=1\n",
        "                    covis_by_store[st][a][b]+=1;    covis_by_store[st][b][a]+=1\n",
        "                    covis_by_custtype[ct][a][b]+=1; covis_by_custtype[ct][b][a]+=1\n",
        "\n",
        "        if rows % 200_000 == 0:\n",
        "            print(f\"Processed {rows:,} rows...\")\n",
        "\n",
        "    gtop = [it for it,_ in pop.most_common(1500)]\n",
        "    t1=time.time()\n",
        "    print(f\"Build (subset) â€” Rows: {rows:,} | Orders: {orders:,} | Unique items: {len(pop)} | {(t1-t0)/60:.2f} min\")\n",
        "    return (covis_global, covis_by_channel, covis_by_subch,\n",
        "            covis_by_occ, covis_by_store, covis_by_custtype, pop, gtop)\n",
        "\n",
        "(cg, by_ch, by_sc, by_oc, by_st, by_ct, POP, GLOBAL_TOP) = build_covis(MAX_ROWS_BUILD)\n",
        "\n",
        "# --------- Blended scorer with tunables ----------\n",
        "DEFAULT_W = (0.51, 0.24, 0.05, 0.11, 0.17, 0.10)  # (global, channel, subch, occasion, store, custtype)\n",
        "\n",
        "def blended_scores(cart, row, W, CAND_POOL=120, BACKFILL_ALPHA=0.15):\n",
        "    sc = Counter()\n",
        "    def add_from(map_, weight):\n",
        "        if weight <= 0: return\n",
        "        for it in cart:\n",
        "            for c, v in map_.get(it, {}).items():\n",
        "                sc[c] += weight * v\n",
        "\n",
        "    # base/global\n",
        "    add_from(cg, W[0])\n",
        "\n",
        "    ch = row.get(\"ORDER_CHANNEL_NAME\",\"\")\n",
        "    sub= row.get(\"ORDER_SUBCHANNEL_NAME\",\"\")\n",
        "    oc = row.get(\"ORDER_OCCASION_NAME\",\"\")\n",
        "    st = row.get(\"STORE_NUMBER\",\"\")\n",
        "    ct = row.get(\"CUSTOMER_TYPE\",\"\")\n",
        "\n",
        "    if ch in by_ch:  add_from(by_ch[ch],  W[1])\n",
        "    if sub in by_sc: add_from(by_sc[sub], W[2])\n",
        "    if oc in by_oc:  add_from(by_oc[oc],  W[3])\n",
        "    if st in by_st:  add_from(by_st[st],  W[4])\n",
        "    if ct in by_ct:  add_from(by_ct[ct],  W[5])\n",
        "\n",
        "    for it in cart: sc.pop(it, None)\n",
        "\n",
        "    if len(sc) < 5:   # sparse backoff\n",
        "        for it in cart:\n",
        "            for c, v in cg.get(it, {}).items():\n",
        "                sc[c] += BACKFILL_ALPHA * v\n",
        "        for it in cart: sc.pop(it, None)\n",
        "\n",
        "    # return top candidates (list)\n",
        "    cands = [c for c,_ in sc.most_common(max(CAND_POOL, 3)) if c not in cart]\n",
        "    # backfill with global top\n",
        "    for g in GLOBAL_TOP:\n",
        "        if len(cands) >= max(CAND_POOL,3): break\n",
        "        if g not in cart and g not in cands:\n",
        "            cands.append(g)\n",
        "    return sc, cands\n",
        "\n",
        "def recommend(cart, row, W, CAND_POOL=120, BACKFILL_ALPHA=0.15, k=3):\n",
        "    scores, cands = blended_scores(cart, row, W, CAND_POOL, BACKFILL_ALPHA)\n",
        "    return (cands[:k] + [\"\",\"\",\"\"])[:k]\n",
        "\n",
        "# --------- Strict LOO eval ----------\n",
        "def eval_strict_looo(W, CAND_POOL=120, BACKFILL_ALPHA=0.15, n_eval=N_EVAL, seed=42):\n",
        "    random.seed(seed)\n",
        "    hits1=[]; hits2=[]; hits3=[]; map3=[]; ndcg3=[]\n",
        "    seen=0\n",
        "    for row in row_iter(ORDER_PATH, max_rows=None):\n",
        "        items = extract_items(row.get(\"ORDERS\",\"\"))\n",
        "        uniq  = list(dict.fromkeys(items))\n",
        "        if len(uniq) < 2:\n",
        "            continue\n",
        "        target = uniq[-1]\n",
        "        cart   = [x for x in uniq if x != target]\n",
        "        if not cart:\n",
        "            continue\n",
        "        R = recommend(cart, row, W, CAND_POOL=CAND_POOL, BACKFILL_ALPHA=BACKOFF_ALPHA)\n",
        "        hits1.append(1.0 if target == R[0] else 0.0)\n",
        "        hits2.append(1.0 if target in R[:2] else 0.0)\n",
        "        hits3.append(1.0 if target in R[:3] else 0.0)\n",
        "        # MAP@3\n",
        "        ap=0.0\n",
        "        for i, r in enumerate(R[:3], start=1):\n",
        "            if r == target: ap = 1.0/i; break\n",
        "        map3.append(ap)\n",
        "        # NDCG@3\n",
        "        dcg = 0.0\n",
        "        for i, r in enumerate(R[:3], start=1):\n",
        "            rel = 1.0 if r == target else 0.0\n",
        "            if rel>0:\n",
        "                dcg += (2**rel - 1)/math.log2(i+1)\n",
        "        ndcg3.append(dcg/1.0)\n",
        "        seen += 1\n",
        "        if seen >= n_eval:\n",
        "            break\n",
        "    return {\n",
        "        \"num_eval\": seen,\n",
        "        \"R1\": round(mean(hits1), 5),\n",
        "        \"R2\": round(mean(hits2), 5),\n",
        "        \"R3\": round(mean(hits3), 5),\n",
        "        \"MAP3\": round(mean(map3), 5),\n",
        "        \"NDCG3\": round(mean(ndcg3), 5),\n",
        "    }\n",
        "\n",
        "# --------- Small grid around best-known values ----------\n",
        "GRID_CHANNEL  = [0.20, 0.24, 0.28]\n",
        "GRID_STORE    = [0.15, 0.18, 0.21]\n",
        "GRID_OCCASION = [0.08, 0.11, 0.14]\n",
        "GRID_CANDPOOL = [100, 120, 160]\n",
        "GRID_BACKOFF  = [0.10, 0.15, 0.20]\n",
        "\n",
        "trials = []\n",
        "t0=time.time()\n",
        "trial_id=0\n",
        "for ch in GRID_CHANNEL:\n",
        "    for st in GRID_STORE:\n",
        "        for oc in GRID_OCCASION:\n",
        "            for cp in GRID_CANDPOOL:\n",
        "                for bk in GRID_BACKOFF:\n",
        "                    W = (0.51, ch, 0.05, oc, st, 0.10)  # keep subch & custtype stable\n",
        "                    BACKOFF_ALPHA = bk\n",
        "                    res = eval_strict_looo(W, CAND_POOL=cp, BACKFILL_ALPHA=BACKOFF_ALPHA, n_eval=N_EVAL)\n",
        "                    trial_id += 1\n",
        "                    trials.append({\n",
        "                        \"trial\": trial_id,\n",
        "                        \"W\": W,\n",
        "                        \"CAND_POOL\": cp,\n",
        "                        \"BACKOFF_ALPHA\": BACKOFF_ALPHA,\n",
        "                        **res\n",
        "                    })\n",
        "                    print(f\"[{trial_id:03d}] W={W} CP={cp} BK={bk} -> R@3={res['R3']:.5f}\")\n",
        "\n",
        "print(f\"Tuning done in {(time.time()-t0)/60:.2f} min | trials={len(trials)}\")\n",
        "\n",
        "# --------- Save report & plot ----------\n",
        "trials_sorted = sorted(trials, key=lambda x: (x[\"R3\"], x[\"MAP3\"], x[\"R1\"]), reverse=True)\n",
        "report_csv = os.path.join(OUT_DIR, \"tuning_report.csv\")\n",
        "with open(report_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=list(trials_sorted[0].keys()))\n",
        "    w.writeheader()\n",
        "    for row in trials_sorted: w.writerow(row)\n",
        "\n",
        "best = trials_sorted[0]\n",
        "with open(os.path.join(OUT_DIR, \"best_config.json\"), \"w\") as f:\n",
        "    json.dump(best, f, indent=2)\n",
        "\n",
        "# simple plot\n",
        "plt.figure(figsize=(8,4.5))\n",
        "plt.plot([t[\"trial\"] for t in trials_sorted], [t[\"R3\"] for t in trials_sorted])\n",
        "plt.xlabel(\"Trial (sorted)\")\n",
        "plt.ylabel(\"Recall@3\")\n",
        "plt.title(\"Micro-tune: Recall@3 across trials\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"tuning_plot.png\"), dpi=160); plt.close()\n",
        "\n",
        "print(\"Best config:\", best)\n",
        "print(\"Saved:\", report_csv, \"and tuning_plot.png & best_config.json in\", OUT_DIR)\n",
        "\n",
        "# --------- Optional: regenerate competition sheet with best config ----------\n",
        "if WRITE_FINAL_XLSX and PANDAS_OK:\n",
        "    print(\"Regenerating Recommendation_Output_MAX.xlsx with tuned config (subset-built maps)...\")\n",
        "    df = pd.read_csv(TEST_PATH, dtype=str, keep_default_na=False)\n",
        "    item_cols = [c for c in df.columns if c.upper().startswith(\"ITEM\")]\n",
        "    p1,p2,p3=[],[],[]\n",
        "    for _, r in df.iterrows():\n",
        "        cart = [str(r[c]).strip() for c in item_cols if r.get(c,\"\")]\n",
        "        recs = recommend(cart, r, tuple(best[\"W\"]), CAND_POOL=int(best[\"CAND_POOL\"]),\n",
        "                         BACKFILL_ALPHA=float(best[\"BACKOFF_ALPHA\"]), k=3)\n",
        "        p1.append(recs[0]); p2.append(recs[1]); p3.append(recs[2])\n",
        "    out=df.copy()\n",
        "    out[\"RECOMMENDATION 1\"]=p1; out[\"RECOMMENDATION 2\"]=p2; out[\"RECOMMENDATION 3\"]=p3\n",
        "    cols = [\"CUSTOMER_ID\",\"ORDER_ID\"] + item_cols + [\"RECOMMENDATION 1\",\"RECOMMENDATION 2\",\"RECOMMENDATION 3\"]\n",
        "    out[cols].to_excel(os.path.join(OUT_DIR, \"Recommendation_Output_MAX.xlsx\"), index=False)\n",
        "    print(\"Wrote:\", os.path.join(OUT_DIR, \"Recommendation_Output_MAX.xlsx\"),\n",
        "          \"(note: tuned on subset-built maps â€” re-run on full data for final lock-in)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2sNZBcEvmHK",
        "outputId": "177fc7e7-a2d5-449b-9390-78644fb4817c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 200,000 rows...\n",
            "Build (subset) â€” Rows: 300,000 | Orders: 300,000 | Unique items: 3171 | 1.24 min\n",
            "[001] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[002] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[003] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[004] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[005] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[006] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[007] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[008] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[009] W=(0.51, 0.2, 0.05, 0.08, 0.15, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[010] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[011] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[012] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[013] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[014] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[015] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[016] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[017] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[018] W=(0.51, 0.2, 0.05, 0.11, 0.15, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[019] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[020] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[021] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[022] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[023] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[024] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[025] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[026] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[027] W=(0.51, 0.2, 0.05, 0.14, 0.15, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[028] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[029] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[030] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[031] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[032] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[033] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[034] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[035] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[036] W=(0.51, 0.2, 0.05, 0.08, 0.18, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[037] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[038] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[039] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[040] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[041] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[042] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[043] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[044] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[045] W=(0.51, 0.2, 0.05, 0.11, 0.18, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[046] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[047] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[048] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[049] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[050] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[051] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[052] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[053] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[054] W=(0.51, 0.2, 0.05, 0.14, 0.18, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[055] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[056] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[057] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[058] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[059] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[060] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[061] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[062] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[063] W=(0.51, 0.2, 0.05, 0.08, 0.21, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[064] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[065] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[066] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[067] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[068] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[069] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[070] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[071] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[072] W=(0.51, 0.2, 0.05, 0.11, 0.21, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[073] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[074] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[075] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[076] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[077] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[078] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[079] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[080] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[081] W=(0.51, 0.2, 0.05, 0.14, 0.21, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[082] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[083] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[084] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[085] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[086] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[087] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[088] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[089] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[090] W=(0.51, 0.24, 0.05, 0.08, 0.15, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[091] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[092] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[093] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[094] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[095] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[096] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[097] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[098] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[099] W=(0.51, 0.24, 0.05, 0.11, 0.15, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[100] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[101] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[102] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[103] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[104] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[105] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[106] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[107] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[108] W=(0.51, 0.24, 0.05, 0.14, 0.15, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[109] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[110] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[111] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[112] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[113] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[114] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[115] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[116] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[117] W=(0.51, 0.24, 0.05, 0.08, 0.18, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[118] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[119] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n",
            "[120] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=100 BK=0.2 -> R@3=0.87067\n",
            "[121] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=120 BK=0.1 -> R@3=0.87067\n",
            "[122] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=120 BK=0.15 -> R@3=0.87067\n",
            "[123] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=120 BK=0.2 -> R@3=0.87067\n",
            "[124] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=160 BK=0.1 -> R@3=0.87067\n",
            "[125] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=160 BK=0.15 -> R@3=0.87067\n",
            "[126] W=(0.51, 0.24, 0.05, 0.11, 0.18, 0.1) CP=160 BK=0.2 -> R@3=0.87067\n",
            "[127] W=(0.51, 0.24, 0.05, 0.14, 0.18, 0.1) CP=100 BK=0.1 -> R@3=0.87067\n",
            "[128] W=(0.51, 0.24, 0.05, 0.14, 0.18, 0.1) CP=100 BK=0.15 -> R@3=0.87067\n"
          ]
        }
      ]
    }
  ]
}